{"activation": "relu",
  "batch_size": 16.0,
  "dropout": 0.2,
  "layer_1": 32,
  "layer_2": 16,
  "learning_rate": 0.0030766812527024,
  "optimizer": "adam",
  "regularization": 0.01}