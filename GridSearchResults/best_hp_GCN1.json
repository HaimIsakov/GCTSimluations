{"learning_rate": 0.001, "batch_size": 8, "dropout": 0.5, "activation": "relu", "regularization": 0.0001, "layer_1": 64, "layer_2": 64, "gcn": 5}