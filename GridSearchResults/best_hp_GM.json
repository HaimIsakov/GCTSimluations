{"learning_rate": 0.001, "batch_size": 8, "dropout": 0.5, "activation": "relu", "regularization": 0.001, "layer_1": 64, "layer_2": 16, "gcn": 20}